{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, symmetries are usually described by $groups$.\n",
    "We can characterize the relationship between a function (such as a neural network layer) and a symmetry group by considering its \\textit{equivariance} properties.\n",
    "A map $f: X \\rightarrow Y$ is said to be equivariant w.r.t. the actions $\\rho:G\\times X\\to X$ and $\\rho':G\\times Y\\to Y$ of a group $G$ on $X$ and $Y$ if\n",
    "$$\n",
    "    f\\Big(\\rho_g(x) \\Big) = \\rho_g' \\Big(f(x)\\Big)\\,\n",
    "$$\n",
    "\n",
    "Reference : arXiv 2203.06153\n",
    "\n",
    "<center width=\"600%\"><img src=\"invariance_vs_equivariance.png\" alt=\"Alternative text\"  width=\"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The equivariant transformation on the graph is defined by \n",
    "$$\\phi \\Big( T_g (x) \\Big) = S_g  \\Big( \\phi (x)\\Big) $$\n",
    "<center width=\"500%\"><img src=\"egnn.png\" alt=\"Alternative text\"  width=\"500px\"></center>\n",
    "\n",
    "The equivariant graph convolutional layer EGCL (ref : 2102.09844) is defined by as following \n",
    "\n",
    "$$h^{l+1}, x^{l+1} = EGCL(h^l, x^l, \\mathcal{E})$$\n",
    "\n",
    "It happens over following steps\n",
    "\n",
    "$$m_{ij} = \\phi_e \\Bigg( h_i^l, h_j^l, ||x_i^l - x_j^l||^2, a_{ij}\\Bigg)$$\n",
    "$$x_i^{l+1} = x_i^l + C \\sum_{j \\in \\mathcal{N}_i} (x_i^l - x_j^l) ~\\phi_x (m_{ij})$$\n",
    "$$m_i = \\sum_{j \\in \\mathcal{N}_i} m_{ij}$$\n",
    "$$h_i^{l+1} = \\phi_h(h_i^l, m_i)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/RobDHess/Steerable-E3-GNN\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "\n",
    "\n",
    "class EGNNLayer(MessagePassing):\n",
    "    \"\"\"E(n) Equivariant GNN Layer\n",
    "\n",
    "    Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, edge_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1 + edge_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index, edge_attribute):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos, edge_attribute=edge_attribute)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j, edge_attribute):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists, edge_attribute], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)\n",
    "        # NOTE: some papers divide pos_diff by (dists + 1) to stabilise model.\n",
    "        # NOTE: lucidrains clamps pos_diff between some [-n, +n], also for stability.\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"sum\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lorentz equivariant GNN\n",
    "Reference : 2201.08187\n",
    "Proposition : A continuous function \n",
    "\n",
    "<center width=\"700%\"><img src=\"architecture.jpg\" alt=\"Alternative text\"  width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h^{l+1}, x^{l+1} = LGCL(h^l, x^l, \\mathcal{E})$$\n",
    "\n",
    "It happens over following steps\n",
    "\n",
    "$$m_{ij} = \\phi_e \\Bigg( h_i^l, h_j^l, \\psi(||x_i^l - x_j^l||^2), \\psi( \\langle x_i^l, x_j^l\\rangle)\\Bigg)$$\n",
    "$$w_{ij} = \\phi_{m}(m_{ij})$$\n",
    "$$x_i^{l+1} = x_i^l + C \\sum_{j \\in \\mathcal{N}_i}  \\phi_x (m_{ij})~x_j^l $$\n",
    "$$h_i^{l+1} = h_i^l + \\phi_h(h_i^l, \\sum_{j \\in \\mathcal{N}_i} w_{ij} m_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "local = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JetDataset import Jet_Dataset\n",
    "from mlp import build_mlp\n",
    "\n",
    "dataset_path = '/Users/sanmay/Documents/ICTS_SCHOOL/Main_School/JetDataset/'\n",
    "file_name = dataset_path + 'JetClass_example_100k.root' # -- from -- \"https://hqu.web.cern.ch/datasets/JetClass/example/\" #\n",
    "jet_dataset = Jet_Dataset(dataset_path=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = torch.from_numpy(\n",
    "            np.array(\n",
    "                [\n",
    "                    [1.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, -1.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, -1.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, -1.0],\n",
    "                ],\n",
    "                #dtype=np.float32,\n",
    "            ))\n",
    "\n",
    "def innerprod(x1, x2):\n",
    "        return torch.sum(\n",
    "            torch.matmul(x2.T, torch.matmul(minkowski, x1)), dim=1, keepdim=True\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lorentz_GNNLayer(MessagePassing):\n",
    "    \"\"\"E(n) Equivariant GNN Layer\n",
    "\n",
    "    Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, coord_dim,  activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super(Lorentz_GNNLayer, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.coord_dim = coord_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_phi_e = build_mlp(2*emb_dim + 2*coord_dim, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_x = build_mlp(1, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_h = build_mlp(emb_dim+1, 1, features=[3, 4, 2])\n",
    "        self.mlp_phi_m = build_mlp(1, 1, features=[3, 4, 2])\n",
    "        \n",
    "        \n",
    "        self.minkowski = torch.from_numpy(\n",
    "            np.array(\n",
    "                [\n",
    "                    [1.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, -1.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, -1.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, -1.0],\n",
    "                ],\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def psi(self, x):\n",
    "        return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "    def innerprod(self, x1, x2):\n",
    "        return torch.sum(\n",
    "            torch.matmul(x2.T, torch.matmul(self.minkowski, x1)), dim=1, keepdim=True\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index, edge_attribute):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos, edge_attribute=edge_attribute)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, x_i, x_j):\n",
    "        # Compute messages\n",
    "        msg = torch.cat(\n",
    "            [h_i, h_j,\n",
    "                self.psi(self.innerprod(x_i - x_j, x_i - x_j)),\n",
    "                self.psi(self.innerprod(x_i, x_j))\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        phi_x = self.mlp_phi_x(msg) * x_j\n",
    "        \n",
    "        w_ij = self.mlp_phi_m(msg) * msg\n",
    "        return phi_x, w_ij\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        phi_x, w_ij = inputs\n",
    "        # Aggregate messages\n",
    "        x_aggr = scatter(phi_x, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        w_aggr = scatter(w_ij, index, dim=self.node_dim, reduce=\"sum\")\n",
    "        return x_aggr, w_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, x):\n",
    "        x_aggr, w_aggr = aggr_out\n",
    "        upd_x = x + x_aggr\n",
    "        upd_h = h + w_aggr\n",
    "        return upd_x, upd_h\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = torch.randn(4, 1), torch.randn(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3278]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innerprod(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3714],\n",
       "        [-0.2663],\n",
       "        [-0.7747],\n",
       "        [ 0.0447]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7197]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x2.T, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse, to_undirected\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, knn_graph\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_scatter import scatter\n",
    "from torch_cluster import knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=jet_dataset, batch_size=5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_b = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index, batch = gr_b.x, gr_b.edge_index, gr_b.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([172, 16])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (860x16 and 4x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(x, edge_index, batch)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/work_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[56], line 103\u001b[0m, in \u001b[0;36mLorentzInteractionNetwork.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, batch):\n\u001b[0;32m--> 103\u001b[0m     x, edge_attr, u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlorentzinteractionnetwork(\n\u001b[1;32m    104\u001b[0m         x, edge_index, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, batch\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m u\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/work_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/work_env/lib/python3.11/site-packages/torch_geometric/nn/models/meta.py:142\u001b[0m, in \u001b[0;36mMetaLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr, u, batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m col \u001b[39m=\u001b[39m edge_index[\u001b[39m1\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     edge_attr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_model(x[row], x[col], edge_attr, u,\n\u001b[1;32m    143\u001b[0m                                 batch \u001b[39mif\u001b[39;49;00m batch \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m batch[row])\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_model(x, edge_index, edge_attr, u, batch)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/work_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[56], line 40\u001b[0m, in \u001b[0;36mLorentzEdgeBlock.forward\u001b[0;34m(self, src, dest, edge_attr, u, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, dest, edge_attr, u, batch):\n\u001b[1;32m     38\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     39\u001b[0m         [\n\u001b[0;32m---> 40\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minnerprod(src, src),\n\u001b[1;32m     41\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minnerprod(src, dest),\n\u001b[1;32m     42\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsi(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minnerprod(dest, dest)),\n\u001b[1;32m     43\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsi(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minnerprod(src \u001b[39m-\u001b[39m dest, src \u001b[39m-\u001b[39m dest)),\n\u001b[1;32m     44\u001b[0m         ],\n\u001b[1;32m     45\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout shape : \u001b[39m\u001b[39m'\u001b[39m, out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_mlp(out)\n",
      "Cell \u001b[0;32mIn[56], line 34\u001b[0m, in \u001b[0;36mLorentzEdgeBlock.innerprod\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minnerprod\u001b[39m(\u001b[39mself\u001b[39m, x1, x2):\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(\n\u001b[0;32m---> 34\u001b[0m         torch\u001b[39m.\u001b[39mmul(torch\u001b[39m.\u001b[39;49mmatmul(x1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminkowski), x2), \u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (860x16 and 4x4)"
     ]
    }
   ],
   "source": [
    "model(x, edge_index, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
